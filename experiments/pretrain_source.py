#!/usr/bin/env python3
"""
Script for managing source policies for TURRET
- åŠ è½½å·²æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹
- åªåœ¨éœ€è¦æ—¶è®­ç»ƒæ–°æ¨¡å‹
"""

import sys
import os
import torch
import numpy as np
from typing import Dict, Any, List, Optional, Union

# Add current directory to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from configs.base_config import TURRETConfig
from environments import get_standard_robot_env
from training.trainers.ppo_trainer import PPOTrainer
from models.policies.structured_policy import StructuredPolicyNetwork
from utils import setup_logging, save_checkpoint, load_checkpoint


class SimpleValueNetwork(torch.nn.Module):
    """Simple value network for source policy training"""
    
    def __init__(self, input_dim: int, hidden_dims: List[int] = [256, 256]):
        super().__init__()
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.append(torch.nn.Linear(prev_dim, hidden_dim))
            layers.append(torch.nn.ReLU())
            prev_dim = hidden_dim
        
        layers.append(torch.nn.Linear(prev_dim, 1))
        self.network = torch.nn.Sequential(*layers)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)


def create_source_policy(env, config: Union[TURRETConfig, Dict[str, Any]]) -> StructuredPolicyNetwork:
    """Create a structured policy network for source task"""
    # ç»Ÿä¸€é…ç½®å¤„ç†
    if hasattr(config, 'hidden_dim'):
        hidden_dim = config.hidden_dim
        device = config.device
    else:
        hidden_dim = config.get("hidden_dim", 256)
        device = config.get("device", "cpu")
    
    policy_config = {
        "node_observation_dim": env.observation_space.shape[0],
        "hidden_dim": hidden_dim,
        "output_dim": env.action_space.shape[0],
        "shared_across_nodes": True,
        "device": device
    }
    
    return StructuredPolicyNetwork(policy_config)


def find_model_file(robot_type: str, checkpoint_dir: str) -> Optional[str]:
    """æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶ - æ”¯æŒå¤šç§å‘½åçº¦å®š"""
    # å¯èƒ½çš„æ–‡ä»¶åæ ¼å¼
    possible_filenames = [
        f"{robot_type.lower()}_policy.pth",  # ant_policy.pth
        f"{robot_type}_policy.pth",          # Ant_policy.pth  
        f"best_{robot_type}.pth",            # best_Ant.pth
        f"best_{robot_type.lower()}.pth",    # best_ant.pth
        f"{robot_type.lower()}.pth",         # ant.pth
        f"{robot_type}.pth",                 # Ant.pth
    ]
    
    for filename in possible_filenames:
        filepath = os.path.join(checkpoint_dir, filename)
        if os.path.exists(filepath):
            return filepath
    
    return None


def check_pretrained_models(robot_types: List[str], checkpoint_dir: str) -> Dict[str, bool]:
    """æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹æ˜¯å¦å­˜åœ¨"""
    status = {}
    print("\nğŸ” æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹çŠ¶æ€:")
    
    for robot_type in robot_types:
        model_file = find_model_file(robot_type, checkpoint_dir)
        if model_file:
            file_size = os.path.getsize(model_file) / 1024 / 1024  # MB
            status[robot_type] = True
            print(f"  âœ… {robot_type}: æ‰¾åˆ° {os.path.basename(model_file)} ({file_size:.1f} MB)")
        else:
            status[robot_type] = False
            print(f"  âŒ {robot_type}: æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹")
    
    # åˆ—å‡ºç›®å½•ä¸­æ‰€æœ‰.pthæ–‡ä»¶
    all_pth_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]
    if all_pth_files:
        print(f"\nğŸ“ ç›®å½•ä¸­æ‰€æœ‰æ¨¡å‹æ–‡ä»¶: {all_pth_files}")
    
    return status


def load_source_policies(robot_types: List[str], 
                        checkpoint_dir: str,
                        config: Union[TURRETConfig, Dict[str, Any]]) -> List[StructuredPolicyNetwork]:
    """åŠ è½½é¢„è®­ç»ƒæºç­–ç•¥ - ä¸»è¦ä½¿ç”¨è¿™ä¸ªå‡½æ•°"""
    source_policies = []
    
    print(f"\nğŸ”„ åŠ è½½æºç­–ç•¥...")
    
    # ç»Ÿä¸€é…ç½®å¤„ç†
    if hasattr(config, 'device'):
        device = config.device
        hidden_dim = config.hidden_dim
    else:
        device = config.get("device", "cpu")
        hidden_dim = config.get("hidden_dim", 256)
    
    for robot_type in robot_types:
        model_file = find_model_file(robot_type, checkpoint_dir)
        
        if model_file:
            # åŠ è½½å·²æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹
            try:
                checkpoint = load_checkpoint(model_file, device)
                policy_net = create_fallback_policy(config)
                
                # å°è¯•ä¸åŒçš„æƒé‡åŠ è½½æ–¹å¼
                if 'policy_state' in checkpoint:
                    policy_net.load_state_dict(checkpoint['policy_state'])
                elif 'model_state_dict' in checkpoint:
                    policy_net.load_state_dict(checkpoint['model_state_dict'])
                elif 'state_dict' in checkpoint:
                    policy_net.load_state_dict(checkpoint['state_dict'])
                else:
                    # ç›´æ¥åŠ è½½
                    policy_net.load_state_dict(checkpoint)
                
                source_policies.append(policy_net)
                print(f"  âœ… åŠ è½½é¢„è®­ç»ƒç­–ç•¥: {robot_type}")
                
            except Exception as e:
                print(f"  âš ï¸ åŠ è½½ {robot_type} å¤±è´¥: {e}ï¼Œä½¿ç”¨æœªè®­ç»ƒç­–ç•¥")
                source_policies.append(create_fallback_policy(config))
        else:
            # æ²¡æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨æœªè®­ç»ƒçš„ç­–ç•¥
            print(f"  âš ï¸ {robot_type}: æ— é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨æœªè®­ç»ƒç­–ç•¥")
            source_policies.append(create_fallback_policy(config))
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len([p for p in source_policies if p is not None])}/{len(robot_types)} ä¸ªç­–ç•¥")
    return source_policies


def train_source_policy(robot_type: str, 
                       config: TURRETConfig,
                       save_path: str) -> bool:
    """è®­ç»ƒæºç­–ç•¥ - åªåœ¨éœ€è¦æ—¶è°ƒç”¨"""
    
    logger = setup_logging()
    logger.info(f"Training source policy for {robot_type}")
    
    try:
        # Create environment
        StandardRobotEnv = get_standard_robot_env()
        env = StandardRobotEnv({
            "robot_type": robot_type,
            "max_episode_steps": config.max_episode_steps
        })
        
        # Create policy and value networks
        policy_net = create_source_policy(env, config)
        value_net = SimpleValueNetwork(env.observation_space.shape[0])
        
        # Create trainer
        trainer = PPOTrainer(policy_net, value_net, config)
        
        # Training loop
        best_reward = -float('inf')
        episode_rewards = []
        
        for episode in range(config.total_episodes):
            # Collect experience
            observation, info = env.reset()
            episode_reward = 0
            
            while True:
                # Get action
                with torch.no_grad():
                    obs_tensor = torch.FloatTensor(observation).unsqueeze(0)
                    mean, std = policy_net(obs_tensor)
                    action_dist = torch.distributions.Normal(mean, std)
                    action = action_dist.sample()
                    log_prob = action_dist.log_prob(action).sum(dim=-1)
                    value = value_net(obs_tensor).squeeze(-1)
                
                # Take action
                next_observation, reward, terminated, truncated, info = env.step(
                    action.squeeze(0).numpy()
                )
                
                # Store experience
                trainer.experience_buffer.add(
                    observation, action.squeeze(0).numpy(), reward,
                    next_observation, terminated, truncated,
                    log_prob.item(), value.item()
                )
                
                # Update
                observation = next_observation
                episode_reward += reward
                
                if terminated or truncated:
                    break
            
            # Log episode
            trainer.log_episode(episode_reward, 0)
            episode_rewards.append(episode_reward)
            
            # Train periodically
            if episode % 10 == 0 and len(trainer.experience_buffer) >= 64:
                batch = trainer.experience_buffer.sample(64)
                stats = trainer.train_step(batch)
                
                if episode % 100 == 0:
                    logger.info(f"Episode {episode}: reward={episode_reward:.2f}, "
                               f"policy_loss={stats['policy_loss']:.4f}")
            
            # Save best model
            if episode_reward > best_reward:
                best_reward = episode_reward
                checkpoint = {
                    'policy_state': policy_net.state_dict(),
                    'value_state': value_net.state_dict(),
                    'episode': episode,
                    'reward': episode_reward,
                    'config': config.__dict__
                }
                save_checkpoint(checkpoint, os.path.join(save_path, f"best_{robot_type}.pth"))
            
            # Early stopping
            if len(episode_rewards) >= 100 and np.mean(episode_rewards[-100:]) > 3000:
                logger.info(f"Early stopping: achieved target reward for {robot_type}")
                break
        
        env.close()
        
        # Save final model
        final_checkpoint = {
            'policy_state': policy_net.state_dict(),
            'value_state': value_net.state_dict(),
            'final_episode': episode,
            'final_reward': episode_reward,
            'config': config.__dict__
        }
        save_checkpoint(final_checkpoint, os.path.join(save_path, f"final_{robot_type}.pth"))
        
        logger.info(f"Completed training for {robot_type}. Best reward: {best_reward:.2f}")
        return True
        
    except Exception as e:
        logger.error(f"Failed to train {robot_type}: {e}")
        return False


def create_fallback_policy(config: Union[TURRETConfig, Dict[str, Any]]) -> StructuredPolicyNetwork:
    """åˆ›å»ºå›é€€ç­–ç•¥ï¼ˆæœªè®­ç»ƒï¼‰"""
    # ç»Ÿä¸€é…ç½®å¤„ç†
    if hasattr(config, 'hidden_dim'):
        hidden_dim = config.hidden_dim
        device = config.device
    else:
        hidden_dim = config.get("hidden_dim", 256)
        device = config.get("device", "cpu")
    
    return StructuredPolicyNetwork({
        "node_observation_dim": 10,  # é»˜è®¤å€¼
        "hidden_dim": hidden_dim,
        "output_dim": 6,  # é»˜è®¤å€¼
        "shared_across_nodes": True,
        "device": device
    })


def create_simple_value_network(input_dim: int) -> torch.nn.Module:
    """Create a simple value network for experiments"""
    return torch.nn.Sequential(
        torch.nn.Linear(input_dim, 256),
        torch.nn.ReLU(),
        torch.nn.Linear(256, 256),
        torch.nn.ReLU(),
        torch.nn.Linear(256, 1)
    )


def main():
    """ä¸»å‡½æ•° - æ£€æŸ¥æ¨¡å‹çŠ¶æ€å¹¶æä¾›è®­ç»ƒé€‰é¡¹"""
    
    # Configuration
    config = TURRETConfig(
        device="cpu",
        total_episodes=500,
        batch_size=64,
        learning_rate=3e-4,
        max_episode_steps=1000,
        hidden_dim=256,
        ppo_epochs=10,
        mini_batch_size=64,
        clip_epsilon=0.2,
        value_coef=0.5,
        entropy_coef=0.01,
        max_grad_norm=0.5,
        buffer_size=10000,
        gamma=0.99,
        gae_lambda=0.95,
        observation_shape=(17,),
        action_shape=(6,)
    )
    
    # Robot types
    robot_types = ["HalfCheetah", "Ant", "Hopper", "Walker2d"]
    save_dir = "data/pretrained/source_policies"
    os.makedirs(save_dir, exist_ok=True)
    
    # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
    model_status = check_pretrained_models(robot_types, save_dir)
    
    # ç»Ÿè®¡
    trained_count = sum(1 for status in model_status.values() if status)
    untrained_count = len(robot_types) - trained_count
    
    print(f"\nğŸ“Š æ¨¡å‹çŠ¶æ€ç»Ÿè®¡:")
    print(f"  âœ… å·²é¢„è®­ç»ƒ: {trained_count} ä¸ª")
    print(f"  âŒ éœ€è¦è®­ç»ƒ: {untrained_count} ä¸ª")
    
    # å¦‚æœæœ‰ç¼ºå¤±çš„æ¨¡å‹ï¼Œè¯¢é—®æ˜¯å¦è®­ç»ƒ
    if untrained_count > 0:
        print(f"\nâš ï¸  æœ‰ {untrained_count} ä¸ªæœºå™¨äººæ²¡æœ‰é¢„è®­ç»ƒæ¨¡å‹")
        response = input("æ˜¯å¦ç°åœ¨è®­ç»ƒç¼ºå¤±çš„æ¨¡å‹? (y/n): ")
        
        if response.lower() == 'y':
            for robot_type, has_model in model_status.items():
                if not has_model:
                    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ: {robot_type}")
                    success = train_source_policy(robot_type, config, save_dir)
                    if success:
                        print(f"âœ… å®Œæˆè®­ç»ƒ: {robot_type}")
                    else:
                        print(f"âŒ è®­ç»ƒå¤±è´¥: {robot_type}")
        else:
            print("è·³è¿‡è®­ç»ƒï¼Œä½¿ç”¨æœªè®­ç»ƒçš„ç­–ç•¥è¿›è¡Œå®éªŒ")
    else:
        print("\nğŸ‰ æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹éƒ½å·²å°±ç»ªï¼")


if __name__ == "__main__":
    main()